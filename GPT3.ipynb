{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc42IuT74380",
        "outputId": "69d018c3-421d-4947-91a4-74fa4f692bbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token Embeddings:      25,600,000\n",
            "Positional Embeddings: 262,144\n",
            "Transformer Blocks:    18,886,656\n",
            "Output Projection:     25,600,000\n",
            "----------------------------------------\n",
            "Total Parameters:      70,348,800\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70348800"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "def gpt_parameter_count(\n",
        "    vocab_size=50000,\n",
        "    max_seq_len=512,\n",
        "    d_model=512,\n",
        "    d_ff=None,\n",
        "    n_layers=6,\n",
        "    tied_output=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate total parameters in a GPT-like transformer model.\n",
        "\n",
        "    Args:\n",
        "    - vocab_size (int): Size of the vocabulary.\n",
        "    - max_seq_len (int): Maximum sequence length.\n",
        "    - d_model (int): Embedding/hidden size.\n",
        "    - d_ff (int or None): FFN size (if None, uses 4 * d_model).\n",
        "    - n_layers (int): Number of transformer blocks.\n",
        "    - tied_output (bool): If True, output projection shares weights with input embeddings.\n",
        "\n",
        "    Returns:\n",
        "    - total_params (int): Total number of parameters.\n",
        "    \"\"\"\n",
        "    if d_ff is None:\n",
        "        d_ff = 4 * d_model\n",
        "\n",
        "    # Embedding layers\n",
        "    token_embeddings = vocab_size * d_model\n",
        "    positional_embeddings = max_seq_len * d_model\n",
        "    embedding_params = token_embeddings + positional_embeddings\n",
        "\n",
        "    # Transformer block parameters per layer\n",
        "    mhsa_params_per_layer = 4 * d_model * d_model  # Q, K, V, Out\n",
        "    ffn_params_per_layer = 2 * d_model * d_ff      # Linear1 + Linear2\n",
        "    layernorm_params_per_layer = 4 * d_model       # 2 LayerNorms (gamma + beta)\n",
        "\n",
        "    transformer_params_per_layer = (\n",
        "        mhsa_params_per_layer + ffn_params_per_layer + layernorm_params_per_layer\n",
        "    )\n",
        "\n",
        "    transformer_total = n_layers * transformer_params_per_layer\n",
        "\n",
        "    # Output projection\n",
        "    output_projection = 0 if tied_output else d_model * vocab_size\n",
        "\n",
        "    total_params = embedding_params + transformer_total + output_projection\n",
        "\n",
        "    print(f\"Token Embeddings:      {token_embeddings:,}\")\n",
        "    print(f\"Positional Embeddings: {positional_embeddings:,}\")\n",
        "    print(f\"Transformer Blocks:    {transformer_total:,}\")\n",
        "    print(f\"Output Projection:     {output_projection:,}\")\n",
        "    print(f\"{'-'*40}\")\n",
        "    print(f\"Total Parameters:      {total_params:,}\")\n",
        "\n",
        "    return total_params\n",
        "\n",
        "\n",
        "# Example usage\n",
        "gpt_parameter_count(\n",
        "    vocab_size=50000,\n",
        "    max_seq_len=512,\n",
        "    d_model=512,\n",
        "    n_layers=6,\n",
        "    tied_output=False  # Set True if output weights are shared with input\n",
        ")\n"
      ]
    }
  ]
}